Endee Knowledge assistant
Topic: Semantic Search and retreival Augemnted Generation(RAG) using Endee Vector Datatbase
This project demonstrates how to build a retrieval-first AI system using Endee, a high-performance vector database, as the core semantic engine.

The system supports two closely related but conceptually distinct capabilities:

Semantic Search â€“ retrieving the most relevant document passages using vector similarity

Retrieval-Augmented Generation (RAG) â€“ generating grounded answers by combining retrieval with a language model

A key design principle of this project is that the language model is only used after Endee has selected the most relevant information.
Endee is responsible for what knowledge is relevant; the LLM is responsible only for explaining that knowledge.

ğŸ§  What is Endee?

Endee is a high-performance, open-source vector database designed specifically for semantic similarity search at scale.

Unlike traditional databases that operate on structured data (rows, columns, exact matches), Endee is optimized to:

store high-dimensional vector embeddings

perform approximate nearest-neighbor (ANN) search

retrieve semantically similar data efficiently and reliably

In modern AI systems, Endee acts as the semantic memory layer, enabling applications such as:

Semantic Search

Document Retrieval

Recommendation Systems

Retrieval-Augmented Generation (RAG)

Agentic AI workflows

In this project, Endee is the single source of semantic truth.

ğŸ“ What is a Vector Database?

A vector database is a specialized database built to store and query vector embeddings â€” numerical representations of unstructured data such as:

text

documents

images

audio

code

Embedding models convert data into vectors such that:

semantically similar content â†’ vectors close together

semantically different content â†’ vectors far apart

Vector databases answer questions like:

â€œWhich pieces of data are most similar in meaning to this query?â€

This capability is fundamental to modern AI systems like semantic search and RAG.

ğŸ” How Endee Works (High-Level)

Documents are converted into vector embeddings using an embedding model

These embeddings are stored inside an Endee vector index

When a query arrives:

the query is embedded

Endee performs ANN-based similarity search

Endee returns:

top-K most relevant results

similarity scores

associated metadata (source, page number, etc.)

This enables fast, scalable, and explainable semantic retrieval, even for large document collections.

ğŸ— System Architecture (High Level)

The system is organized into clearly separated layers, each with a single responsibility.

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Interface           â”‚
â”‚ (Streamlit)              â”‚
â”‚ - Query input            â”‚
â”‚ - Mode selection         â”‚
â”‚ - Result display         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application Layer        â”‚
â”‚ (Python Backend)         â”‚
â”‚ - Orchestrates pipeline  â”‚
â”‚ - Handles modes (Search/RAG)
â”‚ - Manages Endee client   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Semantic Layer           â”‚
â”‚ (Embedding Model)        â”‚
â”‚ - Converts text to vectors
â”‚ - Ensures semantic meaning
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector Database          â”‚
â”‚ (Endee)                  â”‚
â”‚ - Stores embeddings      â”‚
â”‚ - Performs similarity search
â”‚ - Returns ranked results â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Generation Layer (RAG)   â”‚
â”‚ (Local LLM â€“ Ollama)     â”‚
â”‚ - Uses retrieved context â”‚
â”‚ - Generates grounded answers
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ” Semantic Search Architecture
Purpose

Retrieve relevant document passages without generation.

Pipeline
User Query
  â†“
Embedding Model
  â†“
Query Vector
  â†“
Endee Vector Database
  â†“
Top-K Similar Vectors
  â†“
Original Text + Metadata + Confidence

Characteristics

Retrieval-only (no LLM)

Fast response time

No hallucination

Fully traceable to documents

Endee is responsible for all relevance decisions.

ğŸ§  RAG (Retrieval-Augmented Generation) Architecture
Purpose

Generate a grounded explanation using retrieved content.

Pipeline
User Question
  â†“
Embedding Model
  â†“
Query Vector
  â†“
Endee Vector Database
  â†“
Top-K Relevant Chunks
  â†“
Prompt Construction
  â†“
Local LLM (Ollama)
  â†“
Final Answer + Citations + Confidence

Key Constraint

The language model only sees content retrieved by Endee.
It never accesses raw documents directly.

This ensures:

grounded answers

reduced hallucination

explainability

ğŸ“„ Document Ingestion Pipeline

Documents are processed before querying and stored semantically.

PDF / TXT Document
  â†“
Text Extraction
  â†“
Page-Aware Chunking (with overlap)
  â†“
Deduplication
  â†“
Embedding Generation
  â†“
Endee Vector Index


Each chunk is stored with metadata:

source file

page number

original text

âš™ï¸ Project Setup & Installation
1ï¸âƒ£ Start Endee (Vector Database)

Endee is run as a standalone service using Docker.

docker-compose.yml
services:
  endee:
    image: endeeio/endee-server:latest
    container_name: endee-server
    ports:
      - "8080:8080"
    environment:
      NDD_AUTH_TOKEN: ""
    volumes:
      - endee-data:/data
    restart: unless-stopped

volumes:
  endee-data:


Start Endee:

docker-compose up -d


Verify:

curl http://localhost:8080/api/v1/index/list


Expected output:

{"indexes":[]}

2ï¸âƒ£ Install Endee Python SDK
pip install endee


The SDK provides:

index management

vector upsert & query

clean abstraction over Endee APIs

3ï¸âƒ£ Configure Endee Client

The backend uses a dedicated client wrapper with safe index handling.

This ensures:

idempotent index creation

automatic recovery after restarts

no manual index management

4ï¸âƒ£ Install Application Dependencies
pip install -r requirements.txt


Key dependencies:

sentence-transformers

streamlit

pypdf

requests

endee

5ï¸âƒ£ Install & Run Ollama (Local LLM)

Install Ollama from:

https://ollama.com


Start the service:

ollama serve


Pull a model:

ollama pull mistral


This model is chosen for:

fast CPU inference

low memory usage

demo-friendly performance

6ï¸âƒ£ Run the Application
streamlit run app.py


Access the UI at:

http://localhost:8501

â–¶ï¸ Execution Order Summary

Start Endee (Docker)

Verify Endee is reachable

Install Python dependencies

Start Ollama

Run the Streamlit app

Endee must always be running before the application starts.

ğŸ”‘ Key Design Choice

Endee is intentionally deployed as a separate service, mirroring real-world production systems where:

vector databases run independently

application services scale separately

retrieval remains stable across restarts
